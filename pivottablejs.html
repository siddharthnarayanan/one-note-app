
<!DOCTYPE html>
<html>
    <head>
        <meta charset="UTF-8">
        <title>PivotTable.js</title>

        <!-- external libs from cdnjs -->
        <link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/c3/0.4.11/c3.min.css">
        <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min.js"></script>
        <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/c3/0.4.11/c3.min.js"></script>
        <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/jquery/1.11.2/jquery.min.js"></script>
        <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/jqueryui/1.11.4/jquery-ui.min.js"></script>
        <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/jquery-csv/0.71/jquery.csv-0.71.min.js"></script>


        <link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/pivottable/2.19.0/pivot.min.css">
        <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/pivottable/2.19.0/pivot.min.js"></script>
        <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/pivottable/2.19.0/d3_renderers.min.js"></script>
        <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/pivottable/2.19.0/c3_renderers.min.js"></script>
        <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/pivottable/2.19.0/export_renderers.min.js"></script>

        <style>
            body {font-family: Verdana;}
            .node {
              border: solid 1px white;
              font: 10px sans-serif;
              line-height: 12px;
              overflow: hidden;
              position: absolute;
              text-indent: 2px;
            }
            .c3-line, .c3-focused {stroke-width: 3px !important;}
            .c3-bar {stroke: white !important; stroke-width: 1;}
            .c3 text { font-size: 12px; color: grey;}
            .tick line {stroke: white;}
            .c3-axis path {stroke: grey;}
            .c3-circle { opacity: 1 !important; }
            .c3-xgrid-focus {visibility: hidden !important;}
        </style>
    </head>
    <body>
        <script type="text/javascript">
            $(function(){
                if(window.location != window.parent.location)
                    $("<a>", {target:"_blank", href:""})
                        .text("[pop out]").prependTo($("body"));

                $("#output").pivotUI(
                    $.csv.toArrays($("#output").text()),
                    $.extend({
                        renderers: $.extend(
                            $.pivotUtilities.renderers,
                            $.pivotUtilities.c3_renderers,
                            $.pivotUtilities.d3_renderers,
                            $.pivotUtilities.export_renderers
                            ),
                        hiddenAttributes: [""]
                    }, {})
                ).show();
             });
        </script>
        <div id="output" style="display: none;">,date,  , ,   
0,2020-02-25,Relativity,"

For floating docs:

Amendment/Other
Should not change batching process
Can be displayed as metadata to the user
DFA Clustering model predicts doc name


Meeting tomorrow with DB team (Brian Ott, Raquel) to determine any further approvals needed
Relativity team to :

Rename fields and consolidate/update data dictionary
Make sure approach - diagram and cases line up




Database write
differences, Columns missing -> ready for review etc.




Datatype ->
pyodbc.DataError: ('22018', ""[22018] [Microsoft][ODBC Driver 17 for SQL Server][SQL Server]Conversion failed when converting the nvarchar value 'No' to data type bit. (245) (SQLExecDirectW)"")


","

for floating docs:

amendment/other
should not change batching process
can be displayed as metadata to the user
dfa clustering model predicts doc name


meeting tomorrow with db team (brian ott, raquel) to determine any further approvals needed
relativity team to :

rename fields and consolidate/update data dictionary
make sure approach - diagram and cases line up




database write
differences, columns missing -> ready for review etc.




datatype ->
pyodbc.dataerror: ('22018', ""[22018] [microsoft][odbc driver 17 for sql server][sql server]conversion failed when converting the nvarchar value 'no' to data type bit. (245) (sqlexecdirectw)"")


"
1,2020-02-26,KIRA,"
Deck

Use-cases
deployment
Kira the fact that it does not seem like it can easily be set up on a mobile server. And a cloud service is something we can't really sell our banking clients




Instance and documents:
Fields that are captured chart-> speak to why some are performing better than others, important for us to know as developers and integrate into our solution
Why didn't it get the first instance?
Fallback
What is Compare for 
Is I used for tracking amended changes
","
deck

use-cases
deployment
kira the fact that it does not seem like it can easily be set up on a mobile server. and a cloud service is something we can't really sell our banking clients




instance and documents:
fields that are captured chart-> speak to why some are performing better than others, important for us to know as developers and integrate into our solution
why didn't it get the first instance?
fallback
what is compare for 
is i used for tracking amended changes
"
2,2020-02-27,Stack: The accelerator is designed as an analytics layer on top of relativity database. We use:,"
Stack: The accelerator is designed as an analytics layer on top of relativity database. We use:

Application: Python
Database: SQL (Relativity database)

 
Project Structure:

Input: Any required static input data
Src (source)

Core document family analysis module: Create document families using extractions + metadata
Clustering module: Embeddings + clustering for documents that could not be linked using extractions + metadata
Database interaction module: Read + Write to Relativity SQL database


Output: output folder for any created files
Log: logs with timestamp
Docs: Documentation on environment setup etc.

","
stack: the accelerator is designed as an analytics layer on top of relativity database. we use:

application: python
database: sql (relativity database)

 
project structure:

input: any required static input data
src (source)

core document family analysis module: create document families using extractions + metadata
clustering module: embeddings + clustering for documents that could not be linked using extractions + metadata
database interaction module: read + write to relativity sql database


output: output folder for any created files
log: logs with timestamp
docs: documentation on environment setup etc.

"
3,2020-02-27,python main.py finetune --pretrained_checkpoint model.pth --train_data data/dicr/train.tsv --val_data data/dicr/dev.tsv,"
python main.py finetune --pretrained_checkpoint model.pth --train_data data/dicr/train.tsv --val_data data/dicr/dev.tsv
","
python main.py finetune --pretrained_checkpoint model.pth --train_data data/dicr/train.tsv --val_data data/dicr/dev.tsv
"
4,2020-02-27,python main.py finetune --pretrained_checkpoint model.pth --train_data data/dicr/train.tsv --val_data data/dicr/dev.tsv,"
[INFO] 2019-09-11 20:16:09 > Run name : BERT-BERT-{phase}-layers_count={layers_count}-hidden_size={hidden_size}-heads_count={heads_count}-{timestamp}-layers_count=1-hidden_size=128-heads_count=2-2019_09_11_20_16_09
[INFO] 2019-09-11 20:16:09 > {'config_path': None, 'data_dir': None, 'train_path': 'data/dicr/train.txt', 'val_path': 'data/dicr/val.txt', 'dictionary_path': 'dictionary.txt', 'checkpoint_dir': None, 'log_output': None, 'dataset_limit': None, 'epochs': 100, 'batch_size': 16, 'print_every': 1, 'save_every': 10, 'vocabulary_size': 30000, 'max_len': 512, 'lr': 0.001, 'clip_grads': False, 'layers_count': 1, 'hidden_size': 128, 'heads_count': 2, 'd_ff': 128, 'dropout_prob': 0.1, 'device': 'cpu', 'function': <function pretrain at 0x7fd23c8ff0d0>}
","
[info] 2019-09-11 20:16:09 > run name : bert-bert-{phase}-layers_count={layers_count}-hidden_size={hidden_size}-heads_count={heads_count}-{timestamp}-layers_count=1-hidden_size=128-heads_count=2-2019_09_11_20_16_09
[info] 2019-09-11 20:16:09 > {'config_path': none, 'data_dir': none, 'train_path': 'data/dicr/train.txt', 'val_path': 'data/dicr/val.txt', 'dictionary_path': 'dictionary.txt', 'checkpoint_dir': none, 'log_output': none, 'dataset_limit': none, 'epochs': 100, 'batch_size': 16, 'print_every': 1, 'save_every': 10, 'vocabulary_size': 30000, 'max_len': 512, 'lr': 0.001, 'clip_grads': false, 'layers_count': 1, 'hidden_size': 128, 'heads_count': 2, 'd_ff': 128, 'dropout_prob': 0.1, 'device': 'cpu', 'function': <function pretrain at 0x7fd23c8ff0d0>}
"
5,2020-02-27,python main.py finetune --pretrained_checkpoint model.pth --train_data data/dicr/train.tsv --val_data data/dicr/dev.tsv,"

","

"
6,2020-02-27,python main.py finetune --pretrained_checkpoint model.pth --train_data data/dicr/train.tsv --val_data data/dicr/dev.tsv,"
python maint.py pretrain --train_data data/dicr/train.txt --val_data data/dicr/val.txt --checkpoint_output model.pth

From <https://github.com/dreamgonfly/BERT-pytorch> 
","
python maint.py pretrain --train_data data/dicr/train.txt --val_data data/dicr/val.txt --checkpoint_output model.pth

from <https://github.com/dreamgonfly/bert-pytorch> 
"
7,2020-02-27,relativity,"
Python DB 
Call

Accelerator 
Walkthrough
Output-> upload
Testing how to persist data back
Use controlnumber to identify right rows
Use update query



Multiple docs
Rashmi-> doll

Any other open items ?

","
python db 
call

accelerator 
walkthrough
output-> upload
testing how to persist data back
use controlnumber to identify right rows
use update query



multiple docs
rashmi-> doll

any other open items ?

"
8,2020-02-27,relativity,"
Relativity TODO
Test accelerator output doc

Accelerator TODO
Test writing back to DB 
ControlNumber is the unique key
","
relativity <h5>todo</h5>
test accelerator output doc

accelerator <h5>todo</h5>
test writing back to db 
controlnumber is the unique key
"
9,2020-02-27,notice,"
The Address field was identified as a must-have field to be captured for the document family accelerator to match linked documents based on addresses for the contracting parties. This was communicated to SEAL in EY's rulebook.

SEAL activated the ""QFC_Notice"" field to try and capture text in the notice section of the documents (which can include the address of the parties) in an effort to capture the addresses mentioned within the text, but the extracted text will require extensive transformation/processing to capture the accurate address value. SEAL currently cannot extract this and has also confirmed that a field to specifically extract addresses is not currently in scope.

This poses a risk to the performance of the accelerator as it will be unable to match useful address values for contracting parties to link documents.
","
the address field was identified as a must-have field to be captured for the document family accelerator to match linked documents based on addresses for the contracting parties. this was communicated to seal in ey's rulebook.

seal activated the ""qfc_notice"" field to try and capture text in the notice section of the documents (which can include the address of the parties) in an effort to capture the addresses mentioned within the text, but the extracted text will require extensive transformation/processing to capture the accurate address value. seal currently cannot extract this and has also confirmed that a field to specifically extract addresses is not currently in scope.

this poses a risk to the performance of the accelerator as it will be unable to match useful address values for contracting parties to link documents.
"
10,2020-02-27,SEAL,"
Amendment 
Does not have notice section -> address
","
amendment 
does not have notice section -> address
"
11,2020-02-27,#1 - List of packages that you provide should have included all dependencies (packages and sub-packages). This is relatively a new uncharted territory for the Bank. I am again requesting that team ensure completeness of the prerequisite of the accelerator,"
Contacts, steps, procedure

UAT￼
Deploy


Security

Compile with gary's list


Get the python code into GitHub

2.  Get CICD set up
3.  Complete development within the Wells Fargo network and env. Unfortunately WF will not allow anyone to pass code via email or map any drive to GitHub.
","
contacts, steps, procedure

uat￼
deploy


security

compile with gary's list


get the python code into github

2.  get cicd set up
3.  complete development within the wells fargo network and env. unfortunately wf will not allow anyone to pass code via email or map any drive to github.
"
12,2020-02-27,#1 - List of packages that you provide should have included all dependencies (packages and sub-packages). This is relatively a new uncharted territory for the Bank. I am again requesting that team ensure completeness of the prerequisite of the accelerator,"

#1 - List of packages that you provide should have included all dependencies (packages and sub-packages). This is relatively a new uncharted territory for the Bank. I am again requesting that team ensure completeness of the prerequisite of the accelerators and work with Architecture team to confirm the availability. 
 
If you are making any assumptions, please call them out. We need to avoid future surprises and stay on track. Each time we find something that is not accounted requires a fresh architecture review and that has the potential to impact our key milestones. 
 
EY has provided WF with list of required packages for NLP, ML tasks for current sprint

We provide the main package names (Spacy, numpy etc.) in our list. These come with a standard set of prerequisite sub-packages but any missing sub-packages can still show up during installation process are due to platform differences, env setup 
Packages are installed to be readily imported into Python. 
Any downloaded package source files for will be successfully compiled and installed.


Due to WF installation process and limited access to public repositories, any missing packages have to be identified in WF repo and individually installed. This is true for any sub-packages required by the main packages to be installed.
EY has documented procedure for package installation and can provide expansive snapshot of  packages currently installed on server to successfully run codebase. This can be leveraged to setup new env 


WF Contact:
Sahib, Manish

Timeline for code freeze:
Steps:
Complete development within the Wells Fargo network and env:
Get the python code into GitHub: EY has identified approach to install Github desktop client on VDI to push/pull code from required Git repo. Currently testing process

Deployment:  EY team is reviewing process and documentation on the automated CICD pipeline. Sahib to lead deployment steps once EY developers push code base to Git repo

WF contact:
Manish, Sahib

 


I do appreciate all the proactive effort being undertaken by you and the your team in assisting with the installation. 
 
My recommendation is that for EY understand our procedures and requirements of the deployments.
 
Action owner - Gary, Dan and Ran.
 
 
 
#2: Per SDLC all unit test cases have to be documented and delivered to the QA team. These have to be independently verified by the QA team. Please ensure that you get together with Gary and understand the SDLC process.

Steps:
EY will document test cases and work with WF team to align testing process to QA requirements.

WF contact:
 
Action Item: Gary, Abhijit and Ran 
 
#3: What level of security testing has been done? My assumption is that you are confirming to the Bank’s standards. There is training available at the Bank if you need additional information. 
Steps:
Dan, Gary & Manish have been leading process of incorporating external packages into WF env including necessary approvals. EY has also been working to provide alternative approaches to circumvent issues with importing external packages that are not present in WF approved repository. 
WF contact:

Assumptions: 
Steps:


EY has required dev env on VDI to develop and test codebase on WF env








Dan, Gary & Manish 

","

#1 - list of packages that you provide should have included all dependencies (packages and sub-packages). this is relatively a new uncharted territory for the bank. i am again requesting that team ensure completeness of the prerequisite of the accelerators and work with architecture team to confirm the availability. 
 
if you are making any assumptions, please call them out. we need to avoid future surprises and stay on track. each time we find something that is not accounted requires a fresh architecture review and that has the potential to impact our key milestones. 
 
ey has provided wf with list of required packages for nlp, ml tasks for current sprint

we provide the main package names (spacy, numpy etc.) in our list. these come with a standard set of prerequisite sub-packages but any missing sub-packages can still show up during installation process are due to platform differences, env setup 
packages are installed to be readily imported into python. 
any downloaded package source files for will be successfully compiled and installed.


due to wf installation process and limited access to public repositories, any missing packages have to be identified in wf repo and individually installed. this is true for any sub-packages required by the main packages to be installed.
ey has documented procedure for package installation and can provide expansive snapshot of  packages currently installed on server to successfully run codebase. this can be leveraged to setup new env 


wf contact:
sahib, manish

timeline for code freeze:
steps:
complete development within the wells fargo network and env:
get the python code into github: ey has identified approach to install github desktop client on vdi to push/pull code from required git repo. currently testing process

deployment:  ey team is reviewing process and documentation on the automated cicd pipeline. sahib to lead deployment steps once ey developers push code base to git repo

wf contact:
manish, sahib

 


i do appreciate all the proactive effort being undertaken by you and the your team in assisting with the installation. 
 
my recommendation is that for ey understand our procedures and requirements of the deployments.
 
action owner - gary, dan and ran.
 
 
 
#2: per sdlc all unit test cases have to be documented and delivered to the qa team. these have to be independently verified by the qa team. please ensure that you get together with gary and understand the sdlc process.

steps:
ey will document test cases and work with wf team to align testing process to qa requirements.

wf contact:
 
action item: gary, abhijit and ran 
 
#3: what level of security testing has been done? my assumption is that you are confirming to the bank’s standards. there is training available at the bank if you need additional information. 
steps:
dan, gary & manish have been leading process of incorporating external packages into wf env including necessary approvals. ey has also been working to provide alternative approaches to circumvent issues with importing external packages that are not present in wf approved repository. 
wf contact:

assumptions: 
steps:


ey has required dev env on vdi to develop and test codebase on wf env








dan, gary & manish 

"
13,2020-02-26,Multiple docs,"
High level of manual effort
Doc is linked to 6 families
Also terms extraction
Every field needs to be a many-many
In relativity has to be object instead of field
How would you export out now -> challenge

6 families linked 
Which term is linked to 1 family
Not just 1 doc -> cannot trace back to each field

Proposed:
Many-many
Duplicate for each family
6 copies- 6 doc objects
Downstream -> have another field to relate those docs together

How do you know the doc has to be duplicated
ICMP -> 1 doc to multiple families


Doc is loaded to relativity and then find out multiple families 
How to duplicate in relativity
Is someone can duplicate upstream, would be great

Assumption:
6 accounts -> 6 copies -> make 5 more from current -> Joe/Alex/CJ/Scott TODO get confirmation from WF (RC)
Should we use number of accounts / number of entities -> how else would you be able to tell that 

Needs to get duplicated at that level -> data storage level ECMD/NAS
Push data into relativity should be simple -> assign 5 more GUID's to load into relativity


For accelerator:
When SEAL overlays -> concatenate account numbers to 1 record or will this be 3 separate records
When ICMP/index store -> does it have 3 account numbers associated with a document
Doc ID will be same, for account number will they be concatenated or will this be split into multiple rows

Account number -> one of this (Obligation ID + loan ID ..) -> 1 per document -> Assumption
When 1 ICMP send data -> every doc will have 1 record -> 1 doc with 1 account number
1 doc will only have 1 account number
1 account might be mapped to multiple docs




Input 
Assumptions
","
high level of manual effort
doc is linked to 6 families
also terms extraction
every field needs to be a many-many
in relativity has to be object instead of field
how would you export out now -> challenge

6 families linked 
which term is linked to 1 family
not just 1 doc -> cannot trace back to each field

proposed:
many-many
duplicate for each family
6 copies- 6 doc objects
downstream -> have another field to relate those docs together

how do you know the doc has to be duplicated
icmp -> 1 doc to multiple families


doc is loaded to relativity and then find out multiple families 
how to duplicate in relativity
is someone can duplicate upstream, would be great

assumption:
6 accounts -> 6 copies -> make 5 more from current -> joe/alex/cj/scott <h5>todo</h5> get confirmation from wf (rc)
should we use number of accounts / number of entities -> how else would you be able to tell that 

needs to get duplicated at that level -> data storage level ecmd/nas
push data into relativity should be simple -> assign 5 more guid's to load into relativity


for accelerator:
when seal overlays -> concatenate account numbers to 1 record or will this be 3 separate records
when icmp/index store -> does it have 3 account numbers associated with a document
doc id will be same, for account number will they be concatenated or will this be split into multiple rows

account number -> one of this (obligation id + loan id ..) -> 1 per document -> assumption
when 1 icmp send data -> every doc will have 1 record -> 1 doc with 1 account number
1 doc will only have 1 account number
1 account might be mapped to multiple docs




input 
assumptions
"
14,2020-02-26,Address - notice,"
Use zip code/NER location to anchor
Still have to figure out the window
","
use zip code/ner location to anchor
still have to figure out the window
"
15,2020-02-26,DB connection,"
Freeze till end of Feb
Can be Extended
Exception process
We will have to submit an exception -> for it to happen 

Submitted it regularly
Pushing to set it up on WF side
Freeze -> 
IT look a long time, document process

Pending
Firewall issues
If we can wait till end of month
The change can be implemented
Let it run

Internal testing
Wait till Monday

WF:
On our side, we did this
What you will have to do -> challenges we tried to overcome

Otherwise: exception

That change will go through tomorrow

Write


","
freeze till end of feb
can be extended
exception process
we will have to submit an exception -> for it to happen 

submitted it regularly
pushing to set it up on wf side
freeze -> 
it look a long time, document process

pending
firewall issues
if we can wait till end of month
the change can be implemented
let it run

internal testing
wait till monday

wf:
on our side, we did this
what you will have to do -> challenges we tried to overcome

otherwise: exception

that change will go through tomorrow

write


"
16,2020-02-26,scrum,"
Notes: 

Relativity:
Accelerator:

Clustering module currently not operational use to missing Gensim python package
Relativity DB connection module not operational due to pending approvals for DB access


Multi-family doc

Call scheduled later today -> will share discussion points with group following meeting




Install is completed, troubleshooting some issues

 

Git setup: Manish to investigate
Missing packages: Sahib to reach out to Dan
Familiarity and tasks for deployment process 
Latest code shared with Gary via DTS – 

 
","
notes: 

relativity:
accelerator:

clustering module currently not operational use to missing gensim python package
relativity db connection module not operational due to pending approvals for db access


multi-family doc

call scheduled later today -> will share discussion points with group following meeting




install is completed, troubleshooting some issues

 

git setup: manish to investigate
missing packages: sahib to reach out to dan
familiarity and tasks for deployment process 
latest code shared with gary via dts – 

 
"
17,2020-02-25,RSA,"
Fall2020
Ranzhou2 + RSA
 

u749242
","
fall2020
ranzhou2 + rsa
 

u749242
"
18,2020-02-24,Relativity,"

Unassigned docs 

Amendment
other



","

unassigned docs 

amendment
other



"
19,2020-02-24,Relativity,"

","

"
20,2020-02-24,If SEAL overlay,"
If SEAL overlay 
LOB is difficult-> missing for accelerator pilot
Cannot pull from ICMP
MVP0-> mock LOB data -> risk item accelerator

Txt file + folder structure
accountNumber, doc ID, RC

SEAL to consolidate metadata


","
if seal overlay 
lob is difficult-> missing for accelerator pilot
cannot pull from icmp
mvp0-> mock lob data -> risk item accelerator

txt file + folder structure
accountnumber, doc id, rc

seal to consolidate metadata


"
21,2020-02-24,IBM,"
GBS -
Yousef  -> congnitive servcies 
ACD

Documents

Life insurance, disability claims

APS, demand letters, claims data

ACD/ACE

Use cases for collaboration

Who at EY owns IBM relationship
How to partner

EY and watson health already partnered
Have teams follow-up






","
gbs -
yousef  -> congnitive servcies 
acd

documents

life insurance, disability claims

aps, demand letters, claims data

acd/ace

use cases for collaboration

who at ey owns ibm relationship
how to partner

ey and watson health already partnered
have teams follow-up






"
22,2020-02-24,IBM,"
Deal with Watson heath directly
They are taking this to market

In discussion with IBM 
This class of capabilities
Making service available for specific applications 
They haven't tackled insurance yet 

Creating pricing models
Smaller number of bigger relationships
Deeper relationships , not just develop, but partnership

Ben-> offering manager for ACD
Good timing
In talks with other large consulting clients
Insurance -> processes large volume of documents -> makes sense of use-case

Partner
Build applications and revenue share
Lot of ways that we can structure this
We have significant investment in technology

Expertise is not is consulting, flow management
IBM will be tech provider -> they power application  and EY will give application area
We can work to scale 
Datacap is also important in these types of applications
Our customers, our deals, 
IBM cloud service, + any other technology services

Watson anywhere
Running in IBM vs 
Containerized -> platform agnostic
Call it as service
Package and set it up on another env

IBM legacy -> cloud
Fully IPPA enabled -> DB2, cloud object store

Cloud service per volume of text model
Challenge: its takes server customization 

Once you have solid use-case/client
Based on transaction volume
Not some backend transaction system clubbed with support 

Win-win
EY has application area + business knowledge
Applications worth millions dollars, and we can power

EY->proposes
By the document
Easiest way -> to do it

Training our people on using ACD
Wouldn't take too long with background in AI tools + NLP
Access to servers + tools -> can be given instantly

IBM to help demo and show capabilities to EY 
","
deal with watson heath directly
they are taking this to market

in discussion with ibm 
this class of capabilities
making service available for specific applications 
they haven't tackled insurance yet 

creating pricing models
smaller number of bigger relationships
deeper relationships , not just develop, but partnership

ben-> offering manager for acd
good timing
in talks with other large consulting clients
insurance -> processes large volume of documents -> makes sense of use-case

partner
build applications and revenue share
lot of ways that we can structure this
we have significant investment in technology

expertise is not is consulting, flow management
ibm will be tech provider -> they power application  and ey will give application area
we can work to scale 
datacap is also important in these types of applications
our customers, our deals, 
ibm cloud service, + any other technology services

watson anywhere
running in ibm vs 
containerized -> platform agnostic
call it as service
package and set it up on another env

ibm legacy -> cloud
fully ippa enabled -> db2, cloud object store

cloud service per volume of text model
challenge: its takes server customization 

once you have solid use-case/client
based on transaction volume
not some backend transaction system clubbed with support 

win-win
ey has application area + business knowledge
applications worth millions dollars, and we can power

ey->proposes
by the document
easiest way -> to do it

training our people on using acd
wouldn't take too long with background in ai tools + nlp
access to servers + tools -> can be given instantly

ibm to help demo and show capabilities to ey 
"
23,2020-02-24,IBM,"
Next steps: TODO

Document at high level

Solution concept
What do you want to do
Target clients etc.
Make sure IBM has/can add what you need
Size of market, ambitions

Size of opportunity


What % of value is this service
Create win-win deal


Get started:

Pilot pricing 
Limited document types/volume



Gunjan -> NDA needed ? Greg Sarafin’s  team ?

Identify right stake holders TODO


Setup Internal demo -> reach out to Ben at IBM TODO
What ACD does

Layer it up and then go to partners

Business and technical side what ACD does



Yousef -> Will share use-cases


","
next steps: <h5>todo</h5>

document at high level

solution concept
what do you want to do
target clients etc.
make sure ibm has/can add what you need
size of market, ambitions

size of opportunity


what % of value is this service
create win-win deal


get started:

pilot pricing 
limited document types/volume



gunjan -> nda needed ? greg sarafin’s  team ?

identify right stake holders <h5>todo</h5>


setup internal demo -> reach out to ben at ibm <h5>todo</h5>
what acd does

layer it up and then go to partners

business and technical side what acd does



yousef -> will share use-cases


"
24,2020-02-24,IBM,"
Market potential TODO


Setup meeting
Chris, Marcela

Yousef
Will share use-cases

Business and technical side what ACD does

 

","
market potential <h5>todo</h5>


setup meeting
chris, marcela

yousef
will share use-cases

business and technical side what acd does

 

"
25,2020-02-24,"Fredrik is part of IBM’s Offering Management team and is responsible for go-to-market related activities for IBM Watson’s Application and Solutions portfolio. In this role he collaborates with clients, business partners, and IBM marketing and sales to cre","
Form generation

Approach for GTM

Extract info and generate summary

DI + IBM Watson
Pyramid does info extraction

Underwriting
Medical info
Put in risk bucket -> calculate premiums accordingly

Use DI to automate 
Extract from medical docs and automate for underwriters



","
form generation

approach for gtm

extract info and generate summary

di + ibm watson
pyramid does info extraction

underwriting
medical info
put in risk bucket -> calculate premiums accordingly

use di to automate 
extract from medical docs and automate for underwriters



"
26,2020-02-24,"Fredrik is part of IBM’s Offering Management team and is responsible for go-to-market related activities for IBM Watson’s Application and Solutions portfolio. In this role he collaborates with clients, business partners, and IBM marketing and sales to cre","
Fredrik is part of IBM’s Offering Management team and is responsible for go-to-market related activities for IBM Watson’s Application and Solutions portfolio. In this role he collaborates with clients, business partners, and IBM marketing and sales to create and optimize business value, and drive consumption of IBM Watson’s Applications and Solutions.

Prior to this role, Fredrik managed the Client Engagement team for the Watson Client Experience Centers across North America. In this role he was responsible for assisting clients, partners, and media understand the technology and business value of IBM Watson and the IBM Cloud.
Before joining IBM he worked as an industry analyst at Ovum, where he researched and delivered advisory services on trends and strategies in business intelligence, analytics, data visualization, and Big Data.

Throughout his career Fredrik has served in several technical, consultancy, and project management roles. Fredrik has broad technical and strategic understanding of everything from application development and data management to the more high-level human-computer interaction, and has gained expertise in industries ranging from financial services to health to retail.

Fredrik also holds an MSc in Cognitive Neuroscience from Columbia University.


From <https://www.linkedin.com/in/fredriktunvall/> 
","
fredrik is part of ibm’s offering management team and is responsible for go-to-market related activities for ibm watson’s application and solutions portfolio. in this role he collaborates with clients, business partners, and ibm marketing and sales to create and optimize business value, and drive consumption of ibm watson’s applications and solutions.

prior to this role, fredrik managed the client engagement team for the watson client experience centers across north america. in this role he was responsible for assisting clients, partners, and media understand the technology and business value of ibm watson and the ibm cloud.
before joining ibm he worked as an industry analyst at ovum, where he researched and delivered advisory services on trends and strategies in business intelligence, analytics, data visualization, and big data.

throughout his career fredrik has served in several technical, consultancy, and project management roles. fredrik has broad technical and strategic understanding of everything from application development and data management to the more high-level human-computer interaction, and has gained expertise in industries ranging from financial services to health to retail.

fredrik also holds an msc in cognitive neuroscience from columbia university.


from <https://www.linkedin.com/in/fredriktunvall/> 
"
27,2020-02-20,KIRA,"

API and technical: 
KIRA 

End to end upload API
Document intelligence

Specific locations, metadata
Metadata integration

Review 
Export options summary excel files
Pdf exports

Look at API's calls 
ID's, numerical files

What

Batching, 

How will you configure schedule jobs 

AES 26 encryption data 



Meet Ran and Jan about next steps POC evaluation meeting
Check with other users about access
Evaluation criteria sheet
","

api and technical: 
kira 

end to end upload api
document intelligence

specific locations, metadata
metadata integration

review 
export options summary excel files
pdf exports

look at api's calls 
id's, numerical files

what

batching, 

how will you configure schedule jobs 

aes 26 encryption data 



meet ran and jan about next steps poc evaluation meeting
check with other users about access
evaluation criteria sheet
"
28,2020-02-20,KIRA,"
EY:
Teams have created accounts 
Evaluation criteria review undergoing
Sharing with Madrid team, swiss forensics

API &technical: 
KIRA has API documentation + resources
Where it fits in end to end
To check if we can customize, depends on what POC needs
Comparison with other vendors (e.g. SEAL)

Next steps:
Coordination with EMEA team + business team
To decide scope:
Use cases
Types of contracts document types
LIBOR
Contracts seem to be good

CCPA
Capabilities:
Fields by document type, different fields by doc hierarchy as well ? Agreement vs note vs amendment L1, L2, L3

Comparison with other vendors (e.g. SEAL)

Deployment option --- affecting capabilities required
Dev instance, prod instance setup

","
ey:
teams have created accounts 
evaluation criteria review undergoing
sharing with madrid team, swiss forensics

api &technical: 
kira has api documentation + resources
where it fits in end to end
to check if we can customize, depends on what poc needs
comparison with other vendors (e.g. seal)

next steps:
coordination with emea team + business team
to decide scope:
use cases
types of contracts document types
libor
contracts seem to be good

ccpa
capabilities:
fields by document type, different fields by doc hierarchy as well ? agreement vs note vs amendment l1, l2, l3

comparison with other vendors (e.g. seal)

deployment option --- affecting capabilities required
dev instance, prod instance setup

"
29,2020-02-20,KIRA,"
Business case TODO

Co-Develop use case

Scenario
Seal
Kira

Use cases for collaboration
SEAL: currently LIBOR
KIRA: contract heavy documents

KIRA 
does not have outer post-processing layer and rules engine

Rules engine:
Steps Yousef: IBM rules engine

Build solution on top of what we have

License base models + create out IP on top


End tp end
Customoize

ESG meeting

Global vs legal entity collaboration

Consideration


Onboarding on client side with take long if needed

Jonathan inputs:
KIRA
Is there
OCR -> vendor

Model questions -> 
Neural networks

Lease documents
Data sets to train

Comparison between
Dicr vs adam vs kira
Adam-> rules based
Inference time -> DICR takes very long
OCR -> takes very long -> ateast a 100 pages




","
business case <h5>todo</h5>

co-develop use case

scenario
seal
kira

use cases for collaboration
seal: currently libor
kira: contract heavy documents

kira 
does not have outer post-processing layer and rules engine

rules engine:
steps yousef: ibm rules engine

build solution on top of what we have

license base models + create out ip on top


end tp end
customoize

esg meeting

global vs legal entity collaboration

consideration


onboarding on client side with take long if needed

jonathan inputs:
kira
is there
ocr -> vendor

model questions -> 
neural networks

lease documents
data sets to train

comparison between
dicr vs adam vs kira
adam-> rules based
inference time -> dicr takes very long
ocr -> takes very long -> ateast a 100 pages




"
30,2020-02-20,KIRA,"
Slide 1

What is KIRA does
does not have outer post-processing layer and rules engine

Use cases for collaboration
SEAL: currently LIBOR
KIRA: contract heavy documents


Where we can work

Slide 2

Delivery Considerations
Build solution on top of what we have
License base models + create out IP on top


Scenarios

Check what options KIRA offers 
List some for slide pg13-15 in LIBOR workshop ppt


Global vs legal entity collaboration
Onboarding on client side with take long if needed


Slide 3

Insights & Evaluation

Is there OCR vendor
Model questions -> Neural networks

Comparison between


Dicr vs adam vs kira
Adam-> rules based
Inference time -> DICR takes very long
OCR -> takes very long -> ateast a 100 pages







2 slide


","
slide 1

what is kira does
does not have outer post-processing layer and rules engine

use cases for collaboration
seal: currently libor
kira: contract heavy documents


where we can work

slide 2

delivery considerations
build solution on top of what we have
license base models + create out ip on top


scenarios

check what options kira offers 
list some for slide pg13-15 in libor workshop ppt


global vs legal entity collaboration
onboarding on client side with take long if needed


slide 3

insights & evaluation

is there ocr vendor
model questions -> neural networks

comparison between


dicr vs adam vs kira
adam-> rules based
inference time -> dicr takes very long
ocr -> takes very long -> ateast a 100 pages







2 slide


"
31,2020-02-28,KIRA,"
Deck

Use-cases
deployment
Kira the fact that it does not seem like it can easily be set up on a mobile server. And a cloud service is something we can't really sell our banking clients


How to structure a project:
IBM: example : how to partner
Get buy in from other partners at EY, Client

Instance and documents:
Fields that are captured chart-> speak to why some are performing better than others, important for us to know as developers and integrate into our solution
Why didn't it get the first instance?
Fallback
What is Compare for 
Is I used for tracking amended changes
","
deck

use-cases
deployment
kira the fact that it does not seem like it can easily be set up on a mobile server. and a cloud service is something we can't really sell our banking clients


how to structure a project:
ibm: example : how to partner
get buy in from other partners at ey, client

instance and documents:
fields that are captured chart-> speak to why some are performing better than others, important for us to know as developers and integrate into our solution
why didn't it get the first instance?
fallback
what is compare for 
is i used for tracking amended changes
"
32,2020-02-28,KIRA,"
LIBOR

GDPR
Released a few more 
Constantly trying to update with policy
More complicated -> 
Variable: document types and spread
More diversity in documents


Insurance 
Quick study
Medical insurance form:
No-hand written text


On-prem
On solution
Entire stack
Database
Servers 


KIRA extraction:
LIBOR, privacy terms
Still evaluation
Smart fields
Kevin: architecture

On-prem: 

Jan:
Ownership:
Decision

Pricing & GTM:
Few ways to price this
Multi-year : unlimited documents
1 or 2 year documents
Deloitte, PWC

Do you want to own our license

Baseline 
Scale, p

Weekly call:
Update touchpoint





","
libor

gdpr
released a few more 
constantly trying to update with policy
more complicated -> 
variable: document types and spread
more diversity in documents


insurance 
quick study
medical insurance form:
no-hand written text


on-prem
on solution
entire stack
database
servers 


kira extraction:
libor, privacy terms
still evaluation
smart fields
kevin: architecture

on-prem: 

jan:
ownership:
decision

pricing & gtm:
few ways to price this
multi-year : unlimited documents
1 or 2 year documents
deloitte, pwc

do you want to own our license

baseline 
scale, p

weekly call:
update touchpoint





"
33,2020-02-28,KIRA,"
Share list of smart fields KIRA TODO
Analytics team TODO: share document types and provisions that we are looking for in use cases
Analytics team TODO: Connect on pricing and GTM strategy
License, volume, scale

Analytics team TODO: Continue to upload more doc types and use-cases to instance to evaluate models from business requirements and extractions keeping enhancements in mind
","
share list of smart fields kira <h5>todo</h5>
analytics team <h5>todo</h5>: share document types and provisions that we are looking for in use cases
analytics team <h5>todo</h5>: connect on pricing and gtm strategy
license, volume, scale

analytics team <h5>todo</h5>: continue to upload more doc types and use-cases to instance to evaluate models from business requirements and extractions keeping enhancements in mind
"
34,2020-02-28,KIRA,"

KIRA team TODO: Share list of smart fields again: 
Analytics team TODO: Share document types and provisions that we are looking for in use cases
Analytics team TODO: Connect internally on client opportunities, pricing and GTM strategy (License, volume, scale considerations) 
Analytics team TODO: Continue to upload more doc types and use-cases to instance to evaluate models from business requirements and extractions keeping enhancements in mind

 
Key discussion items:

Use-cases: Provided an overview of LIBOR, Privacy and Insurance use cases 
Deployment: On-prem is not a problem. Full stack (DB + application etc.) can be setup at client. Cloud option (AWS etc.) is also available. More discussions later around how models can be integrated into full EY solution
Clarifications on current documents: 
First instance not captured -> Title, dates are currently legacy fields
Fallback: distinction in training between pre/post-2017 language
Recommend training a separate model to capture Legal entities, parties and roles
Compare feature: form and templated documents 

","

kira team <h5>todo</h5>: share list of smart fields again: 
analytics team <h5>todo</h5>: share document types and provisions that we are looking for in use cases
analytics team <h5>todo</h5>: connect internally on client opportunities, pricing and gtm strategy (license, volume, scale considerations) 
analytics team <h5>todo</h5>: continue to upload more doc types and use-cases to instance to evaluate models from business requirements and extractions keeping enhancements in mind

 
key discussion items:

use-cases: provided an overview of libor, privacy and insurance use cases 
deployment: on-prem is not a problem. full stack (db + application etc.) can be setup at client. cloud option (aws etc.) is also available. more discussions later around how models can be integrated into full ey solution
clarifications on current documents: 
first instance not captured -> title, dates are currently legacy fields
fallback: distinction in training between pre/post-2017 language
recommend training a separate model to capture legal entities, parties and roles
compare feature: form and templated documents 

"
35,2020-02-26,KIRA,"

","

"
36,2020-02-27,Server + deployment steps,"
Contacts, steps, procedure

UAT￼
Deploy


Security

Compile with gary's list


Get the python code into GitHub

2.  Get CICD set up
3.  Complete development within the Wells Fargo network and env. Unfortunately WF will not allow anyone to pass code via email or map any drive to GitHub.
","
contacts, steps, procedure

uat￼
deploy


security

compile with gary's list


get the python code into github

2.  get cicd set up
3.  complete development within the wells fargo network and env. unfortunately wf will not allow anyone to pass code via email or map any drive to github.
"
37,2020-02-27,Server + deployment steps,"

#1 - List of packages that you provide should have included all dependencies (packages and sub-packages). This is relatively a new uncharted territory for the Bank. I am again requesting that team ensure completeness of the prerequisite of the accelerators and work with Architecture team to confirm the availability. 
 
If you are making any assumptions, please call them out. We need to avoid future surprises and stay on track. Each time we find something that is not accounted requires a fresh architecture review and that has the potential to impact our key milestones. 
 
EY has provided WF with list of required packages for NLP, ML tasks for current sprint

We provide the main package names (Spacy, numpy etc.) in our list. These come with a standard set of prerequisite sub-packages but any missing sub-packages can still show up during installation process are due to platform differences, env setup 
Packages are installed to be readily imported into Python. 
Any downloaded package source files for will be successfully compiled and installed.


Due to WF installation process and limited access to public repositories, any missing packages have to be identified in WF repo and individually installed. This is true for any sub-packages required by the main packages to be installed.
EY has documented procedure for package installation and can provide expansive snapshot of  packages currently installed on server to successfully run codebase. This can be leveraged to setup new env 


WF Contact:
Sahib, Manish

Timeline for code freeze:
Steps:
Complete development within the Wells Fargo network and env:
Get the python code into GitHub: EY has identified approach to install Github desktop client on VDI to push/pull code from required Git repo. Currently testing process

Deployment:  EY team is reviewing process and documentation on the automated CICD pipeline. Sahib to lead deployment steps once EY developers push code base to Git repo

WF contact:
Manish, Sahib

 


I do appreciate all the proactive effort being undertaken by you and the your team in assisting with the installation. 
 
My recommendation is that for EY understand our procedures and requirements of the deployments.
 
Action owner - Gary, Dan and Ran.
 
 
 
#2: Per SDLC all unit test cases have to be documented and delivered to the QA team. These have to be independently verified by the QA team. Please ensure that you get together with Gary and understand the SDLC process.

Steps:
EY will document test cases and work with WF team to align testing process to QA requirements.

WF contact:
 
Action Item: Gary, Abhijit and Ran 
 
#3: What level of security testing has been done? My assumption is that you are confirming to the Bank’s standards. There is training available at the Bank if you need additional information. 
Steps:
Dan, Gary & Manish have been leading process of incorporating external packages into WF env including necessary approvals. EY has also been working to provide alternative approaches to circumvent issues with importing external packages that are not present in WF approved repository. 
WF contact:

Assumptions: 
Steps:


EY has required dev env on VDI to develop and test codebase on WF env








Dan, Gary & Manish 

","

#1 - list of packages that you provide should have included all dependencies (packages and sub-packages). this is relatively a new uncharted territory for the bank. i am again requesting that team ensure completeness of the prerequisite of the accelerators and work with architecture team to confirm the availability. 
 
if you are making any assumptions, please call them out. we need to avoid future surprises and stay on track. each time we find something that is not accounted requires a fresh architecture review and that has the potential to impact our key milestones. 
 
ey has provided wf with list of required packages for nlp, ml tasks for current sprint

we provide the main package names (spacy, numpy etc.) in our list. these come with a standard set of prerequisite sub-packages but any missing sub-packages can still show up during installation process are due to platform differences, env setup 
packages are installed to be readily imported into python. 
any downloaded package source files for will be successfully compiled and installed.


due to wf installation process and limited access to public repositories, any missing packages have to be identified in wf repo and individually installed. this is true for any sub-packages required by the main packages to be installed.
ey has documented procedure for package installation and can provide expansive snapshot of  packages currently installed on server to successfully run codebase. this can be leveraged to setup new env 


wf contact:
sahib, manish

timeline for code freeze:
steps:
complete development within the wells fargo network and env:
get the python code into github: ey has identified approach to install github desktop client on vdi to push/pull code from required git repo. currently testing process

deployment:  ey team is reviewing process and documentation on the automated cicd pipeline. sahib to lead deployment steps once ey developers push code base to git repo

wf contact:
manish, sahib

 


i do appreciate all the proactive effort being undertaken by you and the your team in assisting with the installation. 
 
my recommendation is that for ey understand our procedures and requirements of the deployments.
 
action owner - gary, dan and ran.
 
 
 
#2: per sdlc all unit test cases have to be documented and delivered to the qa team. these have to be independently verified by the qa team. please ensure that you get together with gary and understand the sdlc process.

steps:
ey will document test cases and work with wf team to align testing process to qa requirements.

wf contact:
 
action item: gary, abhijit and ran 
 
#3: what level of security testing has been done? my assumption is that you are confirming to the bank’s standards. there is training available at the bank if you need additional information. 
steps:
dan, gary & manish have been leading process of incorporating external packages into wf env including necessary approvals. ey has also been working to provide alternative approaches to circumvent issues with importing external packages that are not present in wf approved repository. 
wf contact:

assumptions: 
steps:


ey has required dev env on vdi to develop and test codebase on wf env








dan, gary & manish 

"
38,2020-02-25,Relativity,"

For floating docs:

Amendment/Other
Should not change batching process
Can be displayed as metadata to the user
DFA Clustering model predicts doc name


Meeting tomorrow with DB team (Brian Ott, Raquel) to determine any further approvals needed
Relativity team to :

Rename fields and consolidate/update data dictionary
Make sure approach - diagram and cases line up




Database write
differences, Columns missing -> ready for review etc.




Datatype ->
pyodbc.DataError: ('22018', ""[22018] [Microsoft][ODBC Driver 17 for SQL Server][SQL Server]Conversion failed when converting the nvarchar value 'No' to data type bit. (245) (SQLExecDirectW)"")


","

for floating docs:

amendment/other
should not change batching process
can be displayed as metadata to the user
dfa clustering model predicts doc name


meeting tomorrow with db team (brian ott, raquel) to determine any further approvals needed
relativity team to :

rename fields and consolidate/update data dictionary
make sure approach - diagram and cases line up




database write
differences, columns missing -> ready for review etc.




datatype ->
pyodbc.dataerror: ('22018', ""[22018] [microsoft][odbc driver 17 for sql server][sql server]conversion failed when converting the nvarchar value 'no' to data type bit. (245) (sqlexecdirectw)"")


"
39,2020-02-27,Stack: The accelerator is designed as an analytics layer on top of relativity database. We use:,"
Stack: The accelerator is designed as an analytics layer on top of relativity database. We use:

Application: Python
Database: SQL (Relativity database)

 
Project Structure:

Input: Any required static input data
Src (source)

Core document family analysis module: Create document families using extractions + metadata
Clustering module: Embeddings + clustering for documents that could not be linked using extractions + metadata
Database interaction module: Read + Write to Relativity SQL database


Output: output folder for any created files
Log: logs with timestamp
Docs: Documentation on environment setup etc.

","
stack: the accelerator is designed as an analytics layer on top of relativity database. we use:

application: python
database: sql (relativity database)

 
project structure:

input: any required static input data
src (source)

core document family analysis module: create document families using extractions + metadata
clustering module: embeddings + clustering for documents that could not be linked using extractions + metadata
database interaction module: read + write to relativity sql database


output: output folder for any created files
log: logs with timestamp
docs: documentation on environment setup etc.

"
40,2020-02-27,python main.py finetune --pretrained_checkpoint model.pth --train_data data/dicr/train.tsv --val_data data/dicr/dev.tsv,"
python main.py finetune --pretrained_checkpoint model.pth --train_data data/dicr/train.tsv --val_data data/dicr/dev.tsv
","
python main.py finetune --pretrained_checkpoint model.pth --train_data data/dicr/train.tsv --val_data data/dicr/dev.tsv
"
41,2020-02-27,python main.py finetune --pretrained_checkpoint model.pth --train_data data/dicr/train.tsv --val_data data/dicr/dev.tsv,"
[INFO] 2019-09-11 20:16:09 > Run name : BERT-BERT-{phase}-layers_count={layers_count}-hidden_size={hidden_size}-heads_count={heads_count}-{timestamp}-layers_count=1-hidden_size=128-heads_count=2-2019_09_11_20_16_09
[INFO] 2019-09-11 20:16:09 > {'config_path': None, 'data_dir': None, 'train_path': 'data/dicr/train.txt', 'val_path': 'data/dicr/val.txt', 'dictionary_path': 'dictionary.txt', 'checkpoint_dir': None, 'log_output': None, 'dataset_limit': None, 'epochs': 100, 'batch_size': 16, 'print_every': 1, 'save_every': 10, 'vocabulary_size': 30000, 'max_len': 512, 'lr': 0.001, 'clip_grads': False, 'layers_count': 1, 'hidden_size': 128, 'heads_count': 2, 'd_ff': 128, 'dropout_prob': 0.1, 'device': 'cpu', 'function': <function pretrain at 0x7fd23c8ff0d0>}
","
[info] 2019-09-11 20:16:09 > run name : bert-bert-{phase}-layers_count={layers_count}-hidden_size={hidden_size}-heads_count={heads_count}-{timestamp}-layers_count=1-hidden_size=128-heads_count=2-2019_09_11_20_16_09
[info] 2019-09-11 20:16:09 > {'config_path': none, 'data_dir': none, 'train_path': 'data/dicr/train.txt', 'val_path': 'data/dicr/val.txt', 'dictionary_path': 'dictionary.txt', 'checkpoint_dir': none, 'log_output': none, 'dataset_limit': none, 'epochs': 100, 'batch_size': 16, 'print_every': 1, 'save_every': 10, 'vocabulary_size': 30000, 'max_len': 512, 'lr': 0.001, 'clip_grads': false, 'layers_count': 1, 'hidden_size': 128, 'heads_count': 2, 'd_ff': 128, 'dropout_prob': 0.1, 'device': 'cpu', 'function': <function pretrain at 0x7fd23c8ff0d0>}
"
42,2020-02-27,python main.py finetune --pretrained_checkpoint model.pth --train_data data/dicr/train.tsv --val_data data/dicr/dev.tsv,"

","

"
43,2020-02-27,python main.py finetune --pretrained_checkpoint model.pth --train_data data/dicr/train.tsv --val_data data/dicr/dev.tsv,"
python maint.py pretrain --train_data data/dicr/train.txt --val_data data/dicr/val.txt --checkpoint_output model.pth

From <https://github.com/dreamgonfly/BERT-pytorch> 
","
python maint.py pretrain --train_data data/dicr/train.txt --val_data data/dicr/val.txt --checkpoint_output model.pth

from <https://github.com/dreamgonfly/bert-pytorch> 
"
44,2020-02-27,relativity,"
Python DB 
Call

Accelerator 
Walkthrough
Output-> upload
Testing how to persist data back
Use controlnumber to identify right rows
Use update query



Multiple docs
Rashmi-> doll

Any other open items ?

","
python db 
call

accelerator 
walkthrough
output-> upload
testing how to persist data back
use controlnumber to identify right rows
use update query



multiple docs
rashmi-> doll

any other open items ?

"
45,2020-02-27,relativity,"
Relativity TODO
Test accelerator output doc

Accelerator TODO
Test writing back to DB 
ControlNumber is the unique key
","
relativity <h5>todo</h5>
test accelerator output doc

accelerator <h5>todo</h5>
test writing back to db 
controlnumber is the unique key
"
46,2020-02-27,notice,"
The Address field was identified as a must-have field to be captured for the document family accelerator to match linked documents based on addresses for the contracting parties. This was communicated to SEAL in EY's rulebook.

SEAL activated the ""QFC_Notice"" field to try and capture text in the notice section of the documents (which can include the address of the parties) in an effort to capture the addresses mentioned within the text, but the extracted text will require extensive transformation/processing to capture the accurate address value. SEAL currently cannot extract this and has also confirmed that a field to specifically extract addresses is not currently in scope.

This poses a risk to the performance of the accelerator as it will be unable to match useful address values for contracting parties to link documents.
","
the address field was identified as a must-have field to be captured for the document family accelerator to match linked documents based on addresses for the contracting parties. this was communicated to seal in ey's rulebook.

seal activated the ""qfc_notice"" field to try and capture text in the notice section of the documents (which can include the address of the parties) in an effort to capture the addresses mentioned within the text, but the extracted text will require extensive transformation/processing to capture the accurate address value. seal currently cannot extract this and has also confirmed that a field to specifically extract addresses is not currently in scope.

this poses a risk to the performance of the accelerator as it will be unable to match useful address values for contracting parties to link documents.
"
47,2020-02-27,SEAL,"
Amendment 
Does not have notice section -> address
","
amendment 
does not have notice section -> address
"
48,2020-02-26,Multiple docs,"
High level of manual effort
Doc is linked to 6 families
Also terms extraction
Every field needs to be a many-many
In relativity has to be object instead of field
How would you export out now -> challenge

6 families linked 
Which term is linked to 1 family
Not just 1 doc -> cannot trace back to each field

Proposed:
Many-many
Duplicate for each family
6 copies- 6 doc objects
Downstream -> have another field to relate those docs together

How do you know the doc has to be duplicated
ICMP -> 1 doc to multiple families


Doc is loaded to relativity and then find out multiple families 
How to duplicate in relativity
Is someone can duplicate upstream, would be great

Assumption:
6 accounts -> 6 copies -> make 5 more from current -> Joe/Alex/CJ/Scott TODO get confirmation from WF (RC)
Should we use number of accounts / number of entities -> how else would you be able to tell that 

Needs to get duplicated at that level -> data storage level ECMD/NAS
Push data into relativity should be simple -> assign 5 more GUID's to load into relativity


For accelerator:
When SEAL overlays -> concatenate account numbers to 1 record or will this be 3 separate records
When ICMP/index store -> does it have 3 account numbers associated with a document
Doc ID will be same, for account number will they be concatenated or will this be split into multiple rows

Account number -> one of this (Obligation ID + loan ID ..) -> 1 per document -> Assumption
When 1 ICMP send data -> every doc will have 1 record -> 1 doc with 1 account number
1 doc will only have 1 account number
1 account might be mapped to multiple docs




Input 
Assumptions
","
high level of manual effort
doc is linked to 6 families
also terms extraction
every field needs to be a many-many
in relativity has to be object instead of field
how would you export out now -> challenge

6 families linked 
which term is linked to 1 family
not just 1 doc -> cannot trace back to each field

proposed:
many-many
duplicate for each family
6 copies- 6 doc objects
downstream -> have another field to relate those docs together

how do you know the doc has to be duplicated
icmp -> 1 doc to multiple families


doc is loaded to relativity and then find out multiple families 
how to duplicate in relativity
is someone can duplicate upstream, would be great

assumption:
6 accounts -> 6 copies -> make 5 more from current -> joe/alex/cj/scott <h5>todo</h5> get confirmation from wf (rc)
should we use number of accounts / number of entities -> how else would you be able to tell that 

needs to get duplicated at that level -> data storage level ecmd/nas
push data into relativity should be simple -> assign 5 more guid's to load into relativity


for accelerator:
when seal overlays -> concatenate account numbers to 1 record or will this be 3 separate records
when icmp/index store -> does it have 3 account numbers associated with a document
doc id will be same, for account number will they be concatenated or will this be split into multiple rows

account number -> one of this (obligation id + loan id ..) -> 1 per document -> assumption
when 1 icmp send data -> every doc will have 1 record -> 1 doc with 1 account number
1 doc will only have 1 account number
1 account might be mapped to multiple docs




input 
assumptions
"
49,2020-02-26,Address - notice,"
Use zip code/NER location to anchor
Still have to figure out the window
","
use zip code/ner location to anchor
still have to figure out the window
"
50,2020-02-26,DB connection,"
Freeze till end of Feb
Can be Extended
Exception process
We will have to submit an exception -> for it to happen 

Submitted it regularly
Pushing to set it up on WF side
Freeze -> 
IT look a long time, document process

Pending
Firewall issues
If we can wait till end of month
The change can be implemented
Let it run

Internal testing
Wait till Monday

WF:
On our side, we did this
What you will have to do -> challenges we tried to overcome

Otherwise: exception

That change will go through tomorrow

Write


","
freeze till end of feb
can be extended
exception process
we will have to submit an exception -> for it to happen 

submitted it regularly
pushing to set it up on wf side
freeze -> 
it look a long time, document process

pending
firewall issues
if we can wait till end of month
the change can be implemented
let it run

internal testing
wait till monday

wf:
on our side, we did this
what you will have to do -> challenges we tried to overcome

otherwise: exception

that change will go through tomorrow

write


"
51,2020-02-26,scrum,"
Notes: 

Relativity:
Accelerator:

Clustering module currently not operational use to missing Gensim python package
Relativity DB connection module not operational due to pending approvals for DB access


Multi-family doc

Call scheduled later today -> will share discussion points with group following meeting




Install is completed, troubleshooting some issues

 

Git setup: Manish to investigate
Missing packages: Sahib to reach out to Dan
Familiarity and tasks for deployment process 
Latest code shared with Gary via DTS – 

 
","
notes: 

relativity:
accelerator:

clustering module currently not operational use to missing gensim python package
relativity db connection module not operational due to pending approvals for db access


multi-family doc

call scheduled later today -> will share discussion points with group following meeting




install is completed, troubleshooting some issues

 

git setup: manish to investigate
missing packages: sahib to reach out to dan
familiarity and tasks for deployment process 
latest code shared with gary via dts – 

 
"
52,2020-02-25,RSA,"
Fall2020
Ranzhou2 + RSA
 

u749242
","
fall2020
ranzhou2 + rsa
 

u749242
"
53,2020-02-24,Relativity,"

Unassigned docs 

Amendment
other



","

unassigned docs 

amendment
other



"
54,2020-02-24,Relativity,"

","

"
55,2020-02-24,If SEAL overlay,"
If SEAL overlay 
LOB is difficult-> missing for accelerator pilot
Cannot pull from ICMP
MVP0-> mock LOB data -> risk item accelerator

Txt file + folder structure
accountNumber, doc ID, RC

SEAL to consolidate metadata


","
if seal overlay 
lob is difficult-> missing for accelerator pilot
cannot pull from icmp
mvp0-> mock lob data -> risk item accelerator

txt file + folder structure
accountnumber, doc id, rc

seal to consolidate metadata


"
56,2020-02-24,IBM,"
GBS -
Yousef  -> congnitive servcies 
ACD

Documents

Life insurance, disability claims

APS, demand letters, claims data

ACD/ACE

Use cases for collaboration

Who at EY owns IBM relationship
How to partner

EY and watson health already partnered
Have teams follow-up






","
gbs -
yousef  -> congnitive servcies 
acd

documents

life insurance, disability claims

aps, demand letters, claims data

acd/ace

use cases for collaboration

who at ey owns ibm relationship
how to partner

ey and watson health already partnered
have teams follow-up






"
57,2020-02-24,IBM,"
Deal with Watson heath directly
They are taking this to market

In discussion with IBM 
This class of capabilities
Making service available for specific applications 
They haven't tackled insurance yet 

Creating pricing models
Smaller number of bigger relationships
Deeper relationships , not just develop, but partnership

Ben-> offering manager for ACD
Good timing
In talks with other large consulting clients
Insurance -> processes large volume of documents -> makes sense of use-case

Partner
Build applications and revenue share
Lot of ways that we can structure this
We have significant investment in technology

Expertise is not is consulting, flow management
IBM will be tech provider -> they power application  and EY will give application area
We can work to scale 
Datacap is also important in these types of applications
Our customers, our deals, 
IBM cloud service, + any other technology services

Watson anywhere
Running in IBM vs 
Containerized -> platform agnostic
Call it as service
Package and set it up on another env

IBM legacy -> cloud
Fully IPPA enabled -> DB2, cloud object store

Cloud service per volume of text model
Challenge: its takes server customization 

Once you have solid use-case/client
Based on transaction volume
Not some backend transaction system clubbed with support 

Win-win
EY has application area + business knowledge
Applications worth millions dollars, and we can power

EY->proposes
By the document
Easiest way -> to do it

Training our people on using ACD
Wouldn't take too long with background in AI tools + NLP
Access to servers + tools -> can be given instantly

IBM to help demo and show capabilities to EY 
","
deal with watson heath directly
they are taking this to market

in discussion with ibm 
this class of capabilities
making service available for specific applications 
they haven't tackled insurance yet 

creating pricing models
smaller number of bigger relationships
deeper relationships , not just develop, but partnership

ben-> offering manager for acd
good timing
in talks with other large consulting clients
insurance -> processes large volume of documents -> makes sense of use-case

partner
build applications and revenue share
lot of ways that we can structure this
we have significant investment in technology

expertise is not is consulting, flow management
ibm will be tech provider -> they power application  and ey will give application area
we can work to scale 
datacap is also important in these types of applications
our customers, our deals, 
ibm cloud service, + any other technology services

watson anywhere
running in ibm vs 
containerized -> platform agnostic
call it as service
package and set it up on another env

ibm legacy -> cloud
fully ippa enabled -> db2, cloud object store

cloud service per volume of text model
challenge: its takes server customization 

once you have solid use-case/client
based on transaction volume
not some backend transaction system clubbed with support 

win-win
ey has application area + business knowledge
applications worth millions dollars, and we can power

ey->proposes
by the document
easiest way -> to do it

training our people on using acd
wouldn't take too long with background in ai tools + nlp
access to servers + tools -> can be given instantly

ibm to help demo and show capabilities to ey 
"
58,2020-02-24,IBM,"
Next steps: TODO

Document at high level

Solution concept
What do you want to do
Target clients etc.
Make sure IBM has/can add what you need
Size of market, ambitions

Size of opportunity


What % of value is this service
Create win-win deal


Get started:

Pilot pricing 
Limited document types/volume



Gunjan -> NDA needed ? Greg Sarafin’s  team ?

Identify right stake holders TODO


Setup Internal demo -> reach out to Ben at IBM TODO
What ACD does

Layer it up and then go to partners

Business and technical side what ACD does



Yousef -> Will share use-cases


","
next steps: <h5>todo</h5>

document at high level

solution concept
what do you want to do
target clients etc.
make sure ibm has/can add what you need
size of market, ambitions

size of opportunity


what % of value is this service
create win-win deal


get started:

pilot pricing 
limited document types/volume



gunjan -> nda needed ? greg sarafin’s  team ?

identify right stake holders <h5>todo</h5>


setup internal demo -> reach out to ben at ibm <h5>todo</h5>
what acd does

layer it up and then go to partners

business and technical side what acd does



yousef -> will share use-cases


"
59,2020-02-24,IBM,"
Market potential TODO


Setup meeting
Chris, Marcela

Yousef
Will share use-cases

Business and technical side what ACD does

 

","
market potential <h5>todo</h5>


setup meeting
chris, marcela

yousef
will share use-cases

business and technical side what acd does

 

"
60,2020-02-24,"Fredrik is part of IBM’s Offering Management team and is responsible for go-to-market related activities for IBM Watson’s Application and Solutions portfolio. In this role he collaborates with clients, business partners, and IBM marketing and sales to cre","
Form generation

Approach for GTM

Extract info and generate summary

DI + IBM Watson
Pyramid does info extraction

Underwriting
Medical info
Put in risk bucket -> calculate premiums accordingly

Use DI to automate 
Extract from medical docs and automate for underwriters



","
form generation

approach for gtm

extract info and generate summary

di + ibm watson
pyramid does info extraction

underwriting
medical info
put in risk bucket -> calculate premiums accordingly

use di to automate 
extract from medical docs and automate for underwriters



"
61,2020-02-24,"Fredrik is part of IBM’s Offering Management team and is responsible for go-to-market related activities for IBM Watson’s Application and Solutions portfolio. In this role he collaborates with clients, business partners, and IBM marketing and sales to cre","
Fredrik is part of IBM’s Offering Management team and is responsible for go-to-market related activities for IBM Watson’s Application and Solutions portfolio. In this role he collaborates with clients, business partners, and IBM marketing and sales to create and optimize business value, and drive consumption of IBM Watson’s Applications and Solutions.

Prior to this role, Fredrik managed the Client Engagement team for the Watson Client Experience Centers across North America. In this role he was responsible for assisting clients, partners, and media understand the technology and business value of IBM Watson and the IBM Cloud.
Before joining IBM he worked as an industry analyst at Ovum, where he researched and delivered advisory services on trends and strategies in business intelligence, analytics, data visualization, and Big Data.

Throughout his career Fredrik has served in several technical, consultancy, and project management roles. Fredrik has broad technical and strategic understanding of everything from application development and data management to the more high-level human-computer interaction, and has gained expertise in industries ranging from financial services to health to retail.

Fredrik also holds an MSc in Cognitive Neuroscience from Columbia University.


From <https://www.linkedin.com/in/fredriktunvall/> 
","
fredrik is part of ibm’s offering management team and is responsible for go-to-market related activities for ibm watson’s application and solutions portfolio. in this role he collaborates with clients, business partners, and ibm marketing and sales to create and optimize business value, and drive consumption of ibm watson’s applications and solutions.

prior to this role, fredrik managed the client engagement team for the watson client experience centers across north america. in this role he was responsible for assisting clients, partners, and media understand the technology and business value of ibm watson and the ibm cloud.
before joining ibm he worked as an industry analyst at ovum, where he researched and delivered advisory services on trends and strategies in business intelligence, analytics, data visualization, and big data.

throughout his career fredrik has served in several technical, consultancy, and project management roles. fredrik has broad technical and strategic understanding of everything from application development and data management to the more high-level human-computer interaction, and has gained expertise in industries ranging from financial services to health to retail.

fredrik also holds an msc in cognitive neuroscience from columbia university.


from <https://www.linkedin.com/in/fredriktunvall/> 
"
</div>
    </body>
</html>
