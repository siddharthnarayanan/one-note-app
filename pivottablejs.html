
<!DOCTYPE html>
<html>
    <head>
        <meta charset="UTF-8">
        <title>PivotTable.js</title>

        <!-- external libs from cdnjs -->
        <link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/c3/0.4.11/c3.min.css">
        <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min.js"></script>
        <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/c3/0.4.11/c3.min.js"></script>
        <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/jquery/1.11.2/jquery.min.js"></script>
        <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/jqueryui/1.11.4/jquery-ui.min.js"></script>
        <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/jquery-csv/0.71/jquery.csv-0.71.min.js"></script>


        <link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/pivottable/2.19.0/pivot.min.css">
        <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/pivottable/2.19.0/pivot.min.js"></script>
        <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/pivottable/2.19.0/d3_renderers.min.js"></script>
        <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/pivottable/2.19.0/c3_renderers.min.js"></script>
        <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/pivottable/2.19.0/export_renderers.min.js"></script>

        <style>
            body {font-family: Verdana;}
            .node {
              border: solid 1px white;
              font: 10px sans-serif;
              line-height: 12px;
              overflow: hidden;
              position: absolute;
              text-indent: 2px;
            }
            .c3-line, .c3-focused {stroke-width: 3px !important;}
            .c3-bar {stroke: white !important; stroke-width: 1;}
            .c3 text { font-size: 12px; color: grey;}
            .tick line {stroke: white;}
            .c3-axis path {stroke: grey;}
            .c3-circle { opacity: 1 !important; }
            .c3-xgrid-focus {visibility: hidden !important;}
        </style>
    </head>
    <body>
        <script type="text/javascript">
            $(function(){
                if(window.location != window.parent.location)
                    $("<a>", {target:"_blank", href:""})
                        .text("[pop out]").prependTo($("body"));

                $("#output").pivotUI(
                    $.csv.toArrays($("#output").text()),
                    $.extend({
                        renderers: $.extend(
                            $.pivotUtilities.renderers,
                            $.pivotUtilities.c3_renderers,
                            $.pivotUtilities.d3_renderers,
                            $.pivotUtilities.export_renderers
                            ),
                        hiddenAttributes: [""]
                    }, {})
                ).show();
             });
        </script>
        <div id="output" style="display: none;">,date,  , ,   
0,2020-02-20,ESG,"
PARAbole

Currently uses DOW Jones API
Contexts within Social, Environment and Governance
News activity is captured using frequency and strength -- not Sentiment analysis 
Disclosures are more important than news
Product mapping -> demo had industry mapping due to some data sharing constraints

Unsupervised learning model
Generates corpus -- standard agnostic SBAS, GCI 
Trains on narratives, new custom one can be readily trained
Pre-trained on 75k documents
Each step is validated at checkpoints 
training is reviewed  manually using a random sample
OCR is ABBYY for scanning. Disclosures are searchable pdf's to directly converted to text
Text is broken into paragraphs using their custom script

UI is simple, actual use as platform
Multiple use cases - LIBOR, tables
Nested tables - they have codebase
Highlights text within document 
PII - classifier -- primary context, secondary context 

Azure, Kubernetes
Readily scalable
On-prem, integratable with AWS
Integrated with SharePoint 

Approach 
Wrap their baseline models into our solution
Augment models if required and business rules, post-processing layer



","
parabole

currently uses dow jones api
contexts within social, environment and governance
news activity is captured using frequency and strength -- not sentiment analysis 
disclosures are more important than news
product mapping -> demo had industry mapping due to some data sharing constraints

unsupervised learning model
generates corpus -- standard agnostic sbas, gci 
trains on narratives, new custom one can be readily trained
pre-trained on 75k documents
each step is validated at checkpoints 
training is reviewed  manually using a random sample
ocr is abbyy for scanning. disclosures are searchable pdf's to directly converted to text
text is broken into paragraphs using their custom script

ui is simple, actual use as platform
multiple use cases - libor, tables
nested tables - they have codebase
highlights text within document 
pii - classifier -- primary context, secondary context 

azure, kubernetes
readily scalable
on-prem, integratable with aws
integrated with sharepoint 

approach 
wrap their baseline models into our solution
augment models if required and business rules, post-processing layer



"
1,2020-02-21,server,"
Only my profile on the server has packages + sample code that runs

WF recommends -> uninstall packages


","
only my profile on the server has packages + sample code that runs

wf recommends -> uninstall packages


"
2,2020-02-20,KIRA,"

API and technical: 
KIRA 

End to end upload API
Document intelligence

Specific locations, metadata
Metadata integration

Review 
Export options summary excel files
Pdf exports

Look at API's calls 
ID's, numerical files

What

Batching, 

How will you configure schedule jobs 

AES 26 encryption data 



Meet Ran and Jan about next steps POC evaluation meeting
Check with other users about access
Evaluation criteria sheet
","

api and technical: 
kira 

end to end upload api
document intelligence

specific locations, metadata
metadata integration

review 
export options summary excel files
pdf exports

look at api's calls 
id's, numerical files

what

batching, 

how will you configure schedule jobs 

aes 26 encryption data 



meet ran and jan about next steps poc evaluation meeting
check with other users about access
evaluation criteria sheet
"
3,2020-02-20,KIRA,"
EY:
Teams have created accounts 
Evaluation criteria review undergoing
Sharing with Madrid team, swiss forensics

API &technical: 
KIRA has API documentation + resources
Where it fits in end to end
To check if we can customize, depends on what POC needs
Comparison with other vendors (e.g. SEAL)

Next steps:
Coordination with EMEA team + business team
To decide scope:
Use cases
Types of contracts document types
LIBOR
Contracts seem to be good

CCPA
Capabilities:
Fields by document type, different fields by doc hierarchy as well ? Agreement vs note vs amendment L1, L2, L3

Comparison with other vendors (e.g. SEAL)

Deployment option --- affecting capabilities required
Dev instance, prod instance setup

","
ey:
teams have created accounts 
evaluation criteria review undergoing
sharing with madrid team, swiss forensics

api &technical: 
kira has api documentation + resources
where it fits in end to end
to check if we can customize, depends on what poc needs
comparison with other vendors (e.g. seal)

next steps:
coordination with emea team + business team
to decide scope:
use cases
types of contracts document types
libor
contracts seem to be good

ccpa
capabilities:
fields by document type, different fields by doc hierarchy as well ? agreement vs note vs amendment l1, l2, l3

comparison with other vendors (e.g. seal)

deployment option --- affecting capabilities required
dev instance, prod instance setup

"
4,2020-02-20,KIRA,"
Business case TODO

Co-Develop use case

Scenario
Seal
Kira

Use cases for collaboration
SEAL: currently LIBOR
KIRA: contract heavy documents

KIRA 
does not have outer post-processing layer and rules engine

Rules engine:
Steps Yousef: IBM rules engine

Build solution on top of what we have

License base models + create out IP on top


End tp end
Customoize

ESG meeting

Global vs legal entity collaboration

Consideration


Onboarding on client side with take long if needed

Jonathan inputs:
KIRA
Is there
OCR -> vendor

Model questions -> 
Neural networks

Lease documents
Data sets to train

Comparison between
Dicr vs adam vs kira
Adam-> rules based
Inference time -> DICR takes very long
OCR -> takes very long -> ateast a 100 pages




","
business case <h5>todo</h5>

co-develop use case

scenario
seal
kira

use cases for collaboration
seal: currently libor
kira: contract heavy documents

kira 
does not have outer post-processing layer and rules engine

rules engine:
steps yousef: ibm rules engine

build solution on top of what we have

license base models + create out ip on top


end tp end
customoize

esg meeting

global vs legal entity collaboration

consideration


onboarding on client side with take long if needed

jonathan inputs:
kira
is there
ocr -> vendor

model questions -> 
neural networks

lease documents
data sets to train

comparison between
dicr vs adam vs kira
adam-> rules based
inference time -> dicr takes very long
ocr -> takes very long -> ateast a 100 pages




"
5,2020-02-21,Relativity server,"
EDF team 
GJ


","
edf team 
gj


"
6,2020-02-19,relativity,"
Follow up from previous meeting:
Dedupe -> within SEAL 
Xml to txt
Rashmi will follow up -- 
SQL DB <-> python on WF

Match CS001, CS001 as same groupID by defult -> assumption*
Or should we keep it as noe
Or should we mark it differently (EX- G_001) to show that we haven't  matched on LE but only due to CS001 being the same


Answered -- *

How is review being done ? 
Look at other docs ?

Business has to tell which level to roll up on 

Text format export- relativity

Final solution on single doc - multiple records
Lester rashmi talk
","
follow up from previous meeting:
dedupe -> within seal 
xml to txt
rashmi will follow up -- 
sql db <-> python on wf

match cs001, cs001 as same groupid by defult -> assumption*
or should we keep it as noe
or should we mark it differently (ex- g_001) to show that we haven't  matched on le but only due to cs001 being the same


answered -- *

how is review being done ? 
look at other docs ?

business has to tell which level to roll up on 

text format export- relativity

final solution on single doc - multiple records
lester rashmi talk
"
7,2020-02-19,relativity,"

","

"
8,2020-02-20,server,"
application server account

sql server local account


Add users to ECMD_admin AD group with DB



Add users to this group
no humans in service account
Create domain account with read privileges TODO

Art request - separate local sql account for accelerator - read/write/update  TODO

Art request to add groups to AD





","
application server account

sql server local account


add users to ecmd_admin ad group with db



add users to this group
no humans in service account
create domain account with read privileges <h5>todo</h5>

art request - separate local sql account for accelerator - read/write/update  <h5>todo</h5>

art request to add groups to ad





"
9,2020-02-20,server,"

","

"
10,2020-02-20,DFA story,"
P1 – approach overview:

Brief description and objective  (e.g. run 20 documents through SEAL vs. Ground truth etc. – can borrow from the daily status email)
Input – also attached our input files: 

Doc samples
Input terms : ICMP Vs. SEAL
Doc Hierarchy Matrix


Four use cases framework
Output – attach the two output files --- After all the discussed changes are determined

Doc Family output (contract set ID, Group ID, completeness check etc.) Vs. indication for Relativity



 
P2 – Results Analysis (comparison of current Vs. target state, in target, how many would be matched by what)  & painpoints (seal extraction results, ICMP data – the one CJ discussed could be a good one to put here, confidence level to be developed, what business requirements should still be identified etc.)
P3 – Areas of improvement & next steps and prioritization items (corresponding to painpoints)
","
p1 – approach overview:

brief description and objective  (e.g. run 20 documents through seal vs. ground truth etc. – can borrow from the daily status email)
input – also attached our input files: 

doc samples
input terms : icmp vs. seal
doc hierarchy matrix


four use cases framework
output – attach the two output files --- after all the discussed changes are determined

doc family output (contract set id, group id, completeness check etc.) vs. indication for relativity



 
p2 – results analysis (comparison of current vs. target state, in target, how many would be matched by what)  & painpoints (seal extraction results, icmp data – the one cj discussed could be a good one to put here, confidence level to be developed, what business requirements should still be identified etc.)
p3 – areas of improvement & next steps and prioritization items (corresponding to painpoints)
"
11,2020-02-19,Storyline,"
Findings

Assessment

ICMP, seal data is not good

% change
Improvement of accuracy from baseline to ideal

How many teams are matched on
","
findings

assessment

icmp, seal data is not good

% change
improvement of accuracy from baseline to ideal

how many teams are matched on
"
12,2020-02-19,Storyline,"
Overall threshold

Date + principal + LE

Give you more false postivei
 not linked by family but date
Even if 1 match -> easier for review
","
overall threshold

date + principal + le

give you more false postivei
 not linked by family but date
even if 1 match -> easier for review
"
13,2020-02-19,Storyline,"
Business input :
Create hierarchy matrix for document types for RC, SBU -> L1,L2,L3
terms to match on + confidence for RC, SBU -> order of importance and weight

UID's for each SBU
Flag in accelerator to set if we should use UID -> determine the level at which level loan/account

Columns align with relativity workflow for linkage
Proposed -> Validated columns for review process
P/C flags for parent/child ID
GroupID -> AccountLevel, ContractSetID -> Loan Level

Current SEAL assessment and  & pre-processing 

We tested on 20 docs from SEAL dev + Linda ICMP

Date Normalization: Current SEAL answer field has no output and interim field has unnormalized dates as well. 
LE: SEAL captures parties in intermediate field 
Principal: Commitment identified. Accelerator expects normalized value
Address: TODO

From current sample: dates & principal especially if they are multiple have most variability -> in tables etc. 

Match on LE -> some will match
Match on date & principal 

Subset of these will match to give most coverage

1 doc -> multiple accounts based on legal entities

4 use cases:
   From sample set used for testing. CRE had metadata for case-1 and consumer did not
   Accuracy of accelerator will improve with expected input for Seal and Metadata -> with agreement-name fuzzy, + normalized date + normalized principal

Account roll up matches on LE between contract sets -> current exact matches only -> need fuzzy TODO
    during batching, an incomplete contract set (CS_002) is sent back in again with new (CS__00N) but (CS_002) is now gone/trashed, will not be used again

ParentChildID works well -> we have simple cases built now and have been building more complicated cases. we may have outliers

Next steps:
Incorporate terms matching + confidence score TODO
Address matching TODO


","
business input :
create hierarchy matrix for document types for rc, sbu -> l1,l2,l3
terms to match on + confidence for rc, sbu -> order of importance and weight

uid's for each sbu
flag in accelerator to set if we should use uid -> determine the level at which level loan/account

columns align with relativity workflow for linkage
proposed -> validated columns for review process
p/c flags for parent/child id
groupid -> accountlevel, contractsetid -> loan level

current seal assessment and  & pre-processing 

we tested on 20 docs from seal dev + linda icmp

date normalization: current seal answer field has no output and interim field has unnormalized dates as well. 
le: seal captures parties in intermediate field 
principal: commitment identified. accelerator expects normalized value
address: <h5>todo</h5>

from current sample: dates & principal especially if they are multiple have most variability -> in tables etc. 

match on le -> some will match
match on date & principal 

subset of these will match to give most coverage

1 doc -> multiple accounts based on legal entities

4 use cases:
   from sample set used for testing. cre had metadata for case-1 and consumer did not
   accuracy of accelerator will improve with expected input for seal and metadata -> with agreement-name fuzzy, + normalized date + normalized principal

account roll up matches on le between contract sets -> current exact matches only -> need fuzzy <h5>todo</h5>
    during batching, an incomplete contract set (cs_002) is sent back in again with new (cs__00n) but (cs_002) is now gone/trashed, will not be used again

parentchildid works well -> we have simple cases built now and have been building more complicated cases. we may have outliers

next steps:
incorporate terms matching + confidence score <h5>todo</h5>
address matching <h5>todo</h5>


"
14,2020-02-06,KIRA,"
asynchronous  API calls

How to embed charts into workspace

Modular, where parts can be leveraged

How long to set up POC
Prod ready



","
asynchronous  api calls

how to embed charts into workspace

modular, where parts can be leveraged

how long to set up poc
prod ready



"
15,2020-02-06,KIRA,"
KIRA 

End to end upload API
Document intelligence

Specific locations, metadata
Metadata integration

Review 
Export options summary excel files
Pdf exports

Look at API's calls 
ID's, numerical files

What

Batching, 

How will you configure schedule jobs 

AES 26 encryption data 



Meet Ran and Jan about next steps POC evaluation meeting
Check with other users about access
Evaluation criteria sheet
","
kira 

end to end upload api
document intelligence

specific locations, metadata
metadata integration

review 
export options summary excel files
pdf exports

look at api's calls 
id's, numerical files

what

batching, 

how will you configure schedule jobs 

aes 26 encryption data 



meet ran and jan about next steps poc evaluation meeting
check with other users about access
evaluation criteria sheet
"
16,2020-02-19,Accelerator,"
Create sample static file for terms to match and confidence score

Test this on accelerator script

TODO


","
create sample static file for terms to match and confidence score

test this on accelerator script

<h5>todo</h5>


"
17,2020-02-19,Business questions,"
CJ questions

Is L1 optional for some types ?

Cases where L2->L3 is the only one where they translate to L1->L2
","
cj questions

is l1 optional for some types ?

cases where l2->l3 is the only one where they translate to l1->l2
"
18,2020-02-19,Business questions,"
UID for loan level or none -- confirm

Double match -- flag this in accelerator TODO




","
uid for loan level or none -- confirm

double match -- flag this in accelerator <h5>todo</h5>




"
19,2020-02-19,Business questions,"
V1 within doc ID 
How to strip ContractID from text


Exact match

CRE + consumer +CMS -> Loan number is contractID




","
v1 within doc id 
how to strip contractid from text


exact match

cre + consumer +cms -> loan number is contractid




"
20,2020-02-19,WIM,"
No fields currently answer
Dates are detected
Not agreement name

Addresses are not captured

Show them contracts
Walk them through fields you're looking for




","
no fields currently answer
dates are detected
not agreement name

addresses are not captured

show them contracts
walk them through fields you're looking for




"
21,2020-02-19,Relativity questions,"
Match by LE mark differently

Match CS001, CS001 as same groupID by defult -> assumption
Or should we keep it as noe
Or should we mark it differently (EX- G_001) to show that we haven't  matched on LE but only due to CS001 being the same

How is review being done ?
Look at other docs ?
","
match by le mark differently

match cs001, cs001 as same groupid by defult -> assumption
or should we keep it as noe
or should we mark it differently (ex- g_001) to show that we haven't  matched on le but only due to cs001 being the same

how is review being done ?
look at other docs ?
"
22,2020-02-19,Relativity questions,"

","

"
23,2020-02-19,SEAL WIM,"
Clearing linkage

Fields:
Date (of clearing agreement)
Broker Name 
Clearing Firm Name
Broker Address
Clearing Firm Address
Date (of Amendment & Schedule A)
Maturity Date

Agreement
No agreement name -> captured in contractType
LE: captured, contract mentions roles but not SEAL
Start date: captured
Address: broker and first clearing address mentioned separately in doc


Schedule A
No agreement name
Start date: captured
Maturity Date: multiple captured correct
LE: captured: need to see who is broker and who is clearing firm
","
clearing linkage

fields:
date (of clearing agreement)
broker name 
clearing firm name
broker address
clearing firm address
date (of amendment & schedule a)
maturity date

agreement
no agreement name -> captured in contracttype
le: captured, contract mentions roles but not seal
start date: captured
address: broker and first clearing address mentioned separately in doc


schedule a
no agreement name
start date: captured
maturity date: multiple captured correct
le: captured: need to see who is broker and who is clearing firm
"
24,2020-02-19,Address -> enhancements,"
Address -> enhancements
","
address -> enhancements
"
25,2020-02-18,relativity,"
Fields <-> relativity database

SEAL
Dedupe scripts

Where dedupe
Where the txt xml script sit ?
TODO accelerator to get back


RC, SBU to be added relativity
TODO accelerator share
TODO excel share static files xls

Load static files into custom objects/ table



","
fields <-> relativity database

seal
dedupe scripts

where dedupe
where the txt xml script sit ?
<h5>todo</h5> accelerator to get back


rc, sbu to be added relativity
<h5>todo</h5> accelerator share
<h5>todo</h5> excel share static files xls

load static files into custom objects/ table



"
</div>
    </body>
</html>
